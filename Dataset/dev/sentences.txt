This article presents an investigation of corpus based methods for the automation of help desk e mail responses
Specifically we investigate this problem along two operational dimensions 1 information gathering technique and 2 granularity of the information
We consider two information gathering techniques retrieval and prediction applied to information represented at two levels of granularity document level and sentence level
Document level methods correspond to the reuse of an existing response e mail to address new requests
Sentence level methods correspond to applying extractive multi document summarization techniques to collate units of information from more than one e mail
Evaluation of the performance of the different methods shows that in combination they are able to successfully automate the generation of responses for a substantial portion of e mail requests in our corpus
We also investigate a meta selection process that learns to choose one method to address a new inquiry e mail thus providing a unified response automation solution
Human syntactic processing shows many signs of taking place within a general purpose short term memory
But this kind of memory is known to have a severely constrained storage capacity — possibly constrained to as few as three or four distinct elements
This article describes a model of syntactic processing that operates successfully within these severe constraints by recognizing constituents in a right corner transformed representation a variant of left corner parsing and mapping this representation to random variables in a Hierarchic Hidden Markov Model a factored time series model which probabilistically models the contents of a bounded memory store over time
Evaluations of the coverage of this model on a large syntactically annotated corpus of English sentences and the accuracy of a a bounded memory parsing strategy based on this model suggest this model may be cognitively plausible
This article proposes ESA a new unsupervised approach to word segmentation
ESA is an iterative process consisting of 3 phases Evaluation Selection and Adjustment
In Evaluation both certainty and uncertainty of character sequence co occurrence in corpora are considered as the statistical evidence supporting goodness measurement
Additionally the statistical data of character sequences with various lengths become comparable with each other by using a simple process called Balancing
In Selection a local maximum strategy is adopted without thresholds and the strategy can be implemented with dynamic programming
In Adjustment a part of the statistical data is updated to improve successive results
In our experiment ESA was evaluated on the SIGHAN Bakeoff-2 data set
The results suggest that ESA is effective on Chinese corpora
It is noteworthy that the F measures of the results are basically monotone increasing and can rapidly converge to relatively high values
Furthermore the empirical formulae based on the results can be used to predict the parameter in ESA to avoid parameter estimation that is usually time consuming
This article describes an approach to Lexical Functional Grammar LFG generation that is based on the fact that the set of strings that an LFG grammar relates to a particular acyclic f structure is a context free language
We present an algorithm that produces for an arbitrary LFG grammar and an arbitrary acyclic input f structure a context free grammar describing exactly the set of strings that the given LFG grammar associates with that f structure
The individual sentences are then available through a standard context free generator operating on that grammar
The context free grammar is constructed by specializing the context free backbone of the LFG grammar for the given f structure and serves as a compact representation of all generation results that the LFG grammar assigns to the input
This approach extends to other grammatical formalisms with explicit context free backbones such as PATR and also to formalisms that permit a context free skeleton to be extracted from richer speciﬁcations
It provides a general mathematical framework for understanding and improving the operation of a family of chart based generation algorithms
Parsing is a key task in natural language processing
It involves predicting for each natural language sentence an abstract representation of the grammatical entities in the sentence and the relations between these entities
This representation provides an interface to compositional semantics and to the notions of “ who did what to whom
” The last two decades have seen great advances in parsing English leading to major leaps also in the performance of applications that use parsers as part of their backbone such as systems for information extraction sentiment analysis text summarization and machine translation
Attempts to replicate the success of parsing English for other languages have often yielded unsatisfactory results
In particular parsing languages with complex word structure and flexible word order has been shown to require non trivial adaptation
This special issue reports on methods that successfully address the challenges involved in parsing a range of morphologically rich languages MRLs
This introduction characterizes MRLs describes the challenges in parsing MRLs and outlines the contributions of the articles in the special issue
These contributions present up to date research efforts that address parsing in varied cross lingual settings
They show that parsing MRLs addresses challenges that transcend particular representational and algorithmic choices
We propose a framework for using multiple sources of linguistic information in the task of identifying multiword expressions in natural language texts
We define various linguistically motivated classification features and introduce novel ways for computing them
We then manually define interrelationships among the features and express them in a Bayesian network
The result is a powerful classifier that can identify multiword expressions of various types and multiple syntactic constructions in text corpora
Our methodology is unsupervised and language independent it requires relatively few language resources and is thus suitable for a large number of languages
We report results on English French and Hebrew and demonstrate a significant improvement in identification accuracy compared with less sophisticated baselines
Linguistic corpus design is a critical concern for building rich annotated corpora useful in different domains of applications
For example speech technologies such as ASR Automatic Speech Recognition or TTS Text to Speech need a huge amount of speech data to train data driven models or to produce synthetic speech
Collecting data is always related to costs recording speech verifying annotations etc
and as a rule of thumb the more data you gather the more costly your application will be
Within this context we present in this article solutions to reduce the amount of linguistic text content while maintaining a sufficient level of linguistic richness required by a model or an application
This problem can be formalized as a Set Covering Problem SCP and we evaluate two algorithmic heuristics applied to design large text corpora in English and French for covering phonological information or POS labels
The first considered algorithm is a standard greedy solution with an agglomerative spitting strategy and we propose a second algorithm based on Lagrangian relaxation
The latter approach provides a lower bound to the cost of each covering solution
This lower bound can be used as a metric to evaluate the quality of a reduced corpus whatever the algorithm applied
Experiments show that a suboptimal algorithm like a greedy algorithm achieves good results the cost of its solutions is not so far from the lower bound about 4.35 for 3 phoneme coverings
Usually constraints in SCP are binary we proposed here a generalization where the constraints on each covering feature can be multi valued
We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees
We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization
This article presents a new model for word sense disambiguation formulated in terms of evolutionary game theory where each word to be disambiguated is represented as a node on a graph whose edges represent word relations and senses are represented as classes
The words simultaneously update their class membership preferences according to the senses that neighboring words are likely to choose
We use distributional information to weigh the influence that each word has on the decisions of the others and semantic similarity information to measure the strength of compatibility among the choices
With this information we can formulate the word sense disambiguation problem as a constraint satisfaction problem and solve it using tools derived from game theory maintaining the textual coherence
The model is based on two ideas Similar words should be assigned to similar classes and the meaning of a word does not depend on all the words in a text but just on some of them
The article provides an in depth motivation of the idea of modeling the word sense disambiguation problem in terms of game theory which is illustrated by an example
The conclusion presents an extensive analysis on the combination of similarity measures to use in the framework and a comparison with state of the art systems
The results show that our model outperforms state of the art algorithms and can be applied to different tasks and in different scenarios
Graphs have a variety of uses in natural language processing particularly as representations of linguistic meaning
A deficit in this area of research is a formal framework for creating combining and using models involving graphs that parallels the frameworks of finite automata for strings and finite tree automata for trees
A possible starting point for such a framework is the formalism of directed acyclic graph DAG automata defined by Kamimura and Slutzki and extended by Quernheim and Knight
In this article we study the latter in depth demonstrating several new results including a practical recognition algorithm that can be used for inference and learning with models defined on DAG automata
We also propose an extension to graphs with unbounded node degree and show that our results carry over to the extended formalism
This article gives an overview of how sentence meaning is represented in eleven deep syntactic frameworks ranging from those based on linguistic theories elaborated for decades to rather lightweight NLP motivated approaches
We outline the most important characteristics of each framework and then discuss how particular language phenomena are treated across those frameworks while trying to shed light on commonalities as well as differences
Steedman 2020 proposes as a formal universal of natural language grammar that grammatical permutations of the kind that have given rise to transformational rules are limited to a class known to mathematicians and computer scientists as the “ separable ” permutations
This class of permutations is exactly the class that can be expressed in combinatory categorial grammars CCGs
The excluded non separable permutations do in fact seem to be absent in a number of studies of crosslinguistic variation in word order in nominal and verbal constructions
The number of permutations that are separable grows in the number n of lexical elements in the construction as the Large Schröder Number Sn−1
Because that number grows much more slowly than the n number of all permutations this generalization is also of considerable practical interest for computational applications such as parsing and machine translation
The present article examines the mathematical and computational origins of this restriction and the reason it is exactly captured in CCG without the imposition of any further constraints