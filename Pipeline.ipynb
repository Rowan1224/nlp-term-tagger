{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd64341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import string\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa3d82d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "VECTOR_PATH = 'glove.6B/glove.6B.50d.txt'\n",
    "EMB_DIMENSION = 50\n",
    "MAX_SEQ_LENGTH = 96 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f3e9c7",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f44e7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"Initialize the attributes of the object of the class.\"\"\"\n",
    "        \n",
    "        # data directory\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        # load text dataset  \n",
    "        self.sentences = self._read_data(data_dir, 'sentences')\n",
    "\n",
    "        # load text dataset  \n",
    "        self.labels = self._read_data(data_dir, 'labels')  \n",
    "        \n",
    "        # load the glove embedding\n",
    "        self.vector_path = VECTOR_PATH\n",
    "        \n",
    "        # set the embedding dimension 50/100/300\n",
    "        self.emb_dimension = EMB_DIMENSION\n",
    "        \n",
    "        \n",
    "        # set the maximum sequence length or max tweet length\n",
    "        self.max_seq_len = MAX_SEQ_LENGTH\n",
    "        \n",
    "        # create the vocabulary from the dataset\n",
    "        self.vocab = sorted(self._create_vocabulary())\n",
    "        \n",
    "        \n",
    "        # map word or tokens to index \n",
    "        self.word_to_index = {word: idx+1 for idx, word in enumerate(sorted(self.vocab))}\n",
    "        \n",
    "        # set pad token index to 0 and unk token index last of vocab\n",
    "        self.word_to_index['[PAD]'] = 0\n",
    "        self.word_to_index['[UNK]'] = len(self.vocab)+1\n",
    "        \n",
    "        # define the entitly labels to index values\n",
    "        self.label_to_index = {'B':0, 'I':1, 'O':2, '[PAD]':-1} \n",
    "\n",
    "        \n",
    "        # create the embedding vector\n",
    "        self.word_embeddings = self._create_embedding()\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the dataset.\"\"\"\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data sample for a given index, along with the lable of the corresponding tweet\"\"\"\n",
    "        \n",
    "        \n",
    "        # - get the data sample corresponding to 'index' (use the list 'self.image_path_list')\n",
    "        data_sample = self.sentences[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        # tokenize the sentence and label\n",
    "        tokens = self._tokenize_text(data_sample)\n",
    "        labels = self._tokenize_text(label)\n",
    "\n",
    "        # use the word_to_index mapping to transform the tokens into indices and save them into an IntTensor\n",
    "        x = torch.IntTensor([self.word_to_index[word] \n",
    "                             if word in self.word_to_index \n",
    "                             else self.word_to_index[\"[UNK]\"] \n",
    "                             for word in tokens])\n",
    "        \n",
    "        # transform the variable to cuda or cpu\n",
    "        x = x.to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # get the index-th label and store it into a FloatTensor\n",
    "        y = [self._label_map(l) for l in labels]\n",
    "        y = torch.IntTensor(torch.stack(y))\n",
    "        # transform the variable to cuda or cpu\n",
    "        y = y.to(device)\n",
    "        # stores the text indices and the label into a dictionary\n",
    "        features = {'token_ids': x, 'labels': y}\n",
    "        \n",
    "        \n",
    "        return features\n",
    "\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        \n",
    "        \"\"\"create a matrix containing word vectors\"\"\"\n",
    "\n",
    "        # load the glove embedding to a dict. token is the key and value is the vector\n",
    "        embeddings_index = {}\n",
    "        with open(self.vector_path,'r') as file:\n",
    "            embeddings_index = {line.split()[0]: np.asarray(line.split()[1], dtype='float32') for line in file}\n",
    "\n",
    "        # create the embedding matrix. keep the words that only present in the dataset. \n",
    "        # each row represent one vector\n",
    "        # row index is the word map index\n",
    "        embedding_matrix = np.zeros((len(self.word_to_index) + 2, self.emb_dimension))\n",
    "        for word, i in self.word_to_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        embedding_matrix[len(self.vocab)+1] = torch.randn(self.emb_dimension)\n",
    "                \n",
    "        return torch.tensor(embedding_matrix, device=device)\n",
    "        \n",
    "        \n",
    "    def _create_vocabulary(self):\n",
    "        \"\"\"Create a vocabulary of unique words from the given text files.\"\"\"\n",
    "        \n",
    "        path = 'vocab.txt'\n",
    "        with open(path, 'r') as file:\n",
    "            vocab = [line.strip() for line in file]\n",
    "\n",
    "        return list(vocab)\n",
    "\n",
    "    def _tokenize_text(self, line):\n",
    "        \"\"\"\n",
    "        Remove non-characters from the text and pads the text to max_seq_len.\n",
    "        *!* Padding is necessary for ensuring that all text_files have the same size\n",
    "        *!* This is required since DataLoader cannot handle tensors of variable length\n",
    "\n",
    "        Return a list of all tokens in the text\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = line.split()\n",
    "        for i in range(self.max_seq_len - len(tokens)):\n",
    "            tokens.append('[PAD]')\n",
    "        return tokens\n",
    "    \n",
    "    def _label_map(self,label,class_num=3):\n",
    "        \n",
    "        \"\"\" convert to labels to one hot vectors\"\"\"\n",
    "        \n",
    "        one_hot = torch.zeros(class_num, dtype=torch.int32)\n",
    "        idx = self.label_to_index[label.upper()]\n",
    "        if idx!=-1:\n",
    "            one_hot[idx] = 1\n",
    "        \n",
    "        return one_hot\n",
    "            \n",
    "            \n",
    "    \n",
    "    def _read_data(self, path, pattern):\n",
    "        \n",
    "        \"\"\" read txt file and return as list of strings\"\"\"\n",
    "        \n",
    "        path = f'{path}/{pattern}.txt'\n",
    "        with open(path, 'r') as file:\n",
    "            data = [line.strip() for line in file]\n",
    "\n",
    "        \n",
    "        return data\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd764f",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86900685",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = EntityDataset('Dataset/train')\n",
    "dataset_test = EntityDataset('Dataset/test')\n",
    "dataset_dev = EntityDataset('Dataset/dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a7ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = dataset_train.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e71d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset_dev, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8510d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 96])\n",
      "torch.Size([8, 96, 3])\n"
     ]
    }
   ],
   "source": [
    "# print an example batch\n",
    "\n",
    "batch_example = next(iter(train_dataloader))\n",
    "tweet_batch_example = batch_example['token_ids']\n",
    "labels_batch_example = batch_example['labels']\n",
    "\n",
    "print(tweet_batch_example.shape)\n",
    "print(labels_batch_example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524a8cc7",
   "metadata": {},
   "source": [
    "### Create RNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7a5904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, word_embeddings,\n",
    "                 max_sequence_length, num_layers, hidden_size, bidirectional, output_size, act_fn):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # embedding layer: converts tokens ids with respectve word vec\n",
    "        self.input_layer = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.input_layer.weight.data = word_embeddings\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size = hidden_size, \n",
    "                           num_layers = num_layers, \n",
    "                           bidirectional=bidirectional, batch_first=True)\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.direction = 2\n",
    "        else:\n",
    "            self.direction = 1\n",
    "            \n",
    "        self.layers = num_layers\n",
    "        \n",
    "        \n",
    "            \n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # output layer\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(self.direction*hidden_size, output_size),act_fn)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # get embedding \n",
    "        emb = self.input_layer(x)\n",
    "        \n",
    "        batch = x.shape[0]\n",
    "        # initialize a hidden state and cell state\n",
    "        h0,c0 = self.init_hidden(batch)\n",
    "        \n",
    "        # get output from lstm layers\n",
    "        l,_ = self.lstm(emb.float(),(h0,c0))\n",
    "        \n",
    "        # flatten the output\n",
    "        l = l.reshape(-1,l.shape[2])\n",
    "    \n",
    "        # get final class probabilities\n",
    "        out = self.output_layer(l)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "                \n",
    "        torch.manual_seed(0)\n",
    "        h0 = torch.randn(self.direction*self.layers, batch_size, self.hidden_size, device=device) \n",
    "        c0 = torch.randn(self.direction*self.layers, batch_size, self.hidden_size, device=device)\n",
    "\n",
    "        return h0,c0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca02cf2",
   "metadata": {},
   "source": [
    "### Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d759c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_tag(labels):\n",
    "    \n",
    "    \"\"\"convert a batch of label indices to list of tags\"\"\"\n",
    "    \n",
    "    #define index to tag mapping\n",
    "    indexMap = {0:'B', 1:'I', 2:'O'}\n",
    "    \n",
    "    #reshape labels to batch_size*MAX_SEQ_LENGTH\n",
    "    labels = labels.reshape((-1,MAX_SEQ_LENGTH))\n",
    "    \n",
    "    batchTags = []\n",
    "    \n",
    "    #convert label index to tags\n",
    "    for batch in labels:\n",
    "    \n",
    "        tags = [indexMap[idx.item()] for idx in batch]\n",
    "        \n",
    "        batchTags.append(tags)\n",
    "    \n",
    "    return batchTags\n",
    "\n",
    "def index_to_token(token_ids):\n",
    "    \n",
    "    \"\"\"convert a batch of token indices to list of strings\"\"\"\n",
    "    \n",
    "    batchSent = []\n",
    "    \n",
    "    for item in token_ids:\n",
    "    \n",
    "        sent = [VOCAB[idx-1] if idx < len(VOCAB) else 'UNK' for idx in item if idx!=0]\n",
    "        \n",
    "        batchSent.append(sent)\n",
    "    \n",
    "    return batchSent\n",
    "\n",
    "\n",
    "def print_predictions(tokens, pred_tags, true_tags):\n",
    "    \n",
    "    \n",
    "    batch_tokens = index_to_token(tokens)\n",
    "      \n",
    "    batch_pred_tags = index_to_tag(pred_tags)\n",
    "    \n",
    "    batch_true_tags = index_to_tag(true_tags)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    from colorama import Fore, Style, Back\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    true = []\n",
    "    \n",
    "    for tokens,true_tags,pred_tags in zip(batch_tokens,batch_pred_tags,batch_true_tags):\n",
    "        \n",
    "        true_tags = true_tags[:len(tokens)]\n",
    "        pred_tags = pred_tags[:len(tokens)]\n",
    "        \n",
    "        output = []\n",
    "    \n",
    "        for t,tl,pl in zip(tokens,true_tags,pred_tags):\n",
    "\n",
    "            assert len(tokens) == len(pred_tags) == len(true_tags)\n",
    "\n",
    "            if tl == pl:\n",
    "                o = f\"{t} {Back.GREEN}[{tl}][{pl}]{Style.RESET_ALL}\"\n",
    "\n",
    "            else:\n",
    "                o = f\"{t} {Back.GREEN}[{tl}]{Style.RESET_ALL}{Back.RED}[{pl}]{Style.RESET_ALL}\"\n",
    "\n",
    "\n",
    "            output.append(o)\n",
    "            \n",
    "        outputs.append(\" \".join(output))\n",
    "        preds.extend(pred_tags)\n",
    "        true.extend(true_tags)\n",
    "    \n",
    "    return outputs, preds, true\n",
    "\n",
    "\n",
    "\n",
    "def eval_lstm(model, eval_dataloader, return_predictions = False):\n",
    "    \n",
    "    model = copy.deepcopy(model)\n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for batch in eval_dataloader:\n",
    "\n",
    "            #get sentences and labels\n",
    "            sent = batch['token_ids']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            \n",
    "            #get number of class or tags\n",
    "            num_class = labels.shape[-1]\n",
    "    \n",
    "            #find the padded tokens\n",
    "            padx = (sent > 0).float()\n",
    "            \n",
    "            #reshape it to make it as the same shape with labels\n",
    "            padx = padx.reshape(-1)\n",
    "            \n",
    "            batch_size = sent.shape[0]\n",
    "            \n",
    "            \n",
    "            #count non-pad tokens\n",
    "            num_tokens = padx.sum().item()\n",
    "        \n",
    "            #count padded tokens\n",
    "            num_pad_tokens = padx.shape[0] - num_tokens\n",
    "            \n",
    "            #reshape it to make it as the same shape with model output\n",
    "            labels = labels.reshape(-1,num_class)\n",
    "            \n",
    "            # Get the predicted labels\n",
    "            y_predicted = model(sent)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            # multiply with padx to ignore padded token predictions \n",
    "            label_predicted = torch.argmax(y_predicted.data, 1)*padx\n",
    "            labels = torch.argmax(labels, 1)*padx\n",
    "            \n",
    "\n",
    "            # Compute accuracy: count the total number of samples,\n",
    "            #and the correct labels (compare the true and predicted labels)\n",
    "            \n",
    "            total_labels += num_tokens #only added the non-padded tokens in count\n",
    "            \n",
    "            # subtract the padded tokens to ignore padded token predictions in final count\n",
    "            correct_labels += ((label_predicted == labels).sum().item() - num_pad_tokens)\n",
    "            \n",
    "            # get output\n",
    "            if return_predictions:\n",
    "                predictions.append(print_predictions(sent,label_predicted,labels))\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    if return_predictions:\n",
    "        return accuracy, predictions\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5603f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, labels):\n",
    "    \n",
    "    #define cross entropy loss \n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    num_class = labels.shape[-1]\n",
    "    \n",
    "    # reshape label to make it similar to model output\n",
    "    labels = labels.reshape(-1,num_class) \n",
    "\n",
    "    #get loss\n",
    "    loss = criterion(outputs, labels.float())\n",
    "    \n",
    "    #get non-pad index\n",
    "    non_pad_index=[i for i in range(labels.shape[0]) if labels[i].sum()!=0]\n",
    "    \n",
    "    #get final loss\n",
    "    loss = loss[non_pad_index].mean()\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "#     #reshape labels to give a flat vector of length batch_size*seq_len\n",
    "#     num_class = labels.shape[-1]\n",
    "    \n",
    "#     # reshape label to make it similar to model output\n",
    "#     labels = labels.reshape(-1,num_class) \n",
    "    \n",
    "\n",
    "#     #the number of non-paded tokens. since padded tokens are labeled as [0,0,0], it doesn't effect count\n",
    "#     num_tokens = int(torch.sum(labels))\n",
    "    \n",
    "\n",
    "#     #pick the values corresponding to labels and multiply by mask\n",
    "#     outputs = outputs*labels\n",
    "\n",
    "#     #cross entropy loss for all non 'PAD' tokens\n",
    "#     return -torch.sum(outputs)/num_tokens\n",
    "      \n",
    "    \n",
    "\n",
    "def training_lstm(model, train_dataloader, valid_dataloader, num_epochs, learning_rate, verbose=True):\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize lists to record the training loss over epochs\n",
    "    loss_all_epochs = []\n",
    "    val_loss_all_epochs = []\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    \n",
    "    accuracy = []\n",
    "    \n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the training loss for the current epoch\n",
    "        loss_current_epoch = 0\n",
    "        val_loss_epoch = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            label = batch['labels']\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            out = model_tr.forward(batch['token_ids'])\n",
    "            l = loss_fn(out,label)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            loss_current_epoch += (l.item())\n",
    "            \n",
    "            val_loss_epoch += loss_fn(out,label).item()\n",
    "            \n",
    "            # - use the 'backward' method to compute the gradients\n",
    "            # - apply the gradient descent algorithm\n",
    "            # Also think of updating the loss at the current epoch\n",
    "\n",
    "\n",
    "        # At the end of each epoch, record and display the loss over all batches in train and val set\n",
    "        loss_current_epoch = loss_current_epoch/len(train_dataloader)\n",
    "        val_loss_epoch = val_loss_epoch/len(train_dataloader)\n",
    "        \n",
    "        loss_all_epochs.append(loss_current_epoch)\n",
    "        val_loss_all_epochs.append(val_loss_epoch)\n",
    "        \n",
    "        # \n",
    "        acc = eval_lstm(model_tr, valid_dataloader)\n",
    "        \n",
    "        accuracy.append(acc)\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            torch.save(model_tr.state_dict(), 'model_opt.pt')\n",
    "            \n",
    "        \n",
    "        \n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss_current_epoch))\n",
    "        \n",
    "    return model_tr, loss_all_epochs ,accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eaa989",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bcc9006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the vocab size that is built from train set\n",
    "vocab_size = len(dataset_train.vocab)\n",
    "# the embedding dimenstion 50/100/300\n",
    "emb_dim = EMB_DIMENSION\n",
    "# get the embedding matrix\n",
    "word_embeddings = dataset_train.word_embeddings\n",
    "# max sequence length\n",
    "max_sequence_length = MAX_SEQ_LENGTH\n",
    "\n",
    "#define lstm layers\n",
    "num_layers = 5\n",
    "#define hidden size\n",
    "hidden_size = 32\n",
    "#set if LSTM should be bidirectional \n",
    "bidirectional = False\n",
    "# output size i.e class size \n",
    "output_size = 3\n",
    "# activation function\n",
    "act_fn = nn.LogSoftmax(dim=1)\n",
    "\n",
    "# create a RNN  model instance. REMARK: remove .cuda() at the end if gpu is not available\n",
    "rnn = RNN(vocab_size, emb_dim, word_embeddings, max_sequence_length, \n",
    "          num_layers,hidden_size, bidirectional, output_size, act_fn).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55980d50",
   "metadata": {},
   "source": [
    "### Trial & Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fcc4998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# o = rnn.forward(tweet_batch_example)\n",
    "# o.shape\n",
    "# loss_fn(o,labels_batch_example)\n",
    "\n",
    "# l =labels_batch_example.reshape(-1,3)\n",
    "# non_pad_index=[i for i in range(l.shape[0]) if l[i].sum()!=0 ]\n",
    "# l = l[non_pad_index]\n",
    "# l.shape\n",
    "\n",
    "\n",
    "# l = labels_batch_example.reshape(-1,3)\n",
    "# l= torch.argmax(l,1)\n",
    "# out = print_predictions(tweet_batch_example,l,l)\n",
    "# print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dafd3b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hossain/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:769: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Loss: 0.7829\n",
      "Epoch [2/75], Loss: 0.6181\n",
      "Epoch [3/75], Loss: 0.4255\n",
      "Epoch [4/75], Loss: 0.3256\n",
      "Epoch [5/75], Loss: 0.2747\n",
      "Epoch [6/75], Loss: 0.2533\n",
      "Epoch [7/75], Loss: 0.2296\n",
      "Epoch [8/75], Loss: 0.2188\n",
      "Epoch [9/75], Loss: 0.2168\n",
      "Epoch [10/75], Loss: 0.2003\n",
      "Epoch [11/75], Loss: 0.1931\n",
      "Epoch [12/75], Loss: 0.1746\n",
      "Epoch [13/75], Loss: 0.1572\n",
      "Epoch [14/75], Loss: 0.1397\n",
      "Epoch [15/75], Loss: 0.1291\n",
      "Epoch [16/75], Loss: 0.1288\n",
      "Epoch [17/75], Loss: 0.1182\n",
      "Epoch [18/75], Loss: 0.1116\n",
      "Epoch [19/75], Loss: 0.1030\n",
      "Epoch [20/75], Loss: 0.0955\n",
      "Epoch [21/75], Loss: 0.0880\n",
      "Epoch [22/75], Loss: 0.0874\n",
      "Epoch [23/75], Loss: 0.0866\n",
      "Epoch [24/75], Loss: 0.0797\n",
      "Epoch [25/75], Loss: 0.0756\n",
      "Epoch [26/75], Loss: 0.0781\n",
      "Epoch [27/75], Loss: 0.0707\n",
      "Epoch [28/75], Loss: 0.0702\n",
      "Epoch [29/75], Loss: 0.0682\n",
      "Epoch [30/75], Loss: 0.0594\n",
      "Epoch [31/75], Loss: 0.0508\n",
      "Epoch [32/75], Loss: 0.0466\n",
      "Epoch [33/75], Loss: 0.0500\n",
      "Epoch [34/75], Loss: 0.0458\n",
      "Epoch [35/75], Loss: 0.0421\n",
      "Epoch [36/75], Loss: 0.0436\n",
      "Epoch [37/75], Loss: 0.0418\n",
      "Epoch [38/75], Loss: 0.0421\n",
      "Epoch [39/75], Loss: 0.0414\n",
      "Epoch [40/75], Loss: 0.0344\n",
      "Epoch [41/75], Loss: 0.0316\n",
      "Epoch [42/75], Loss: 0.0328\n",
      "Epoch [43/75], Loss: 0.0416\n",
      "Epoch [44/75], Loss: 0.0392\n",
      "Epoch [45/75], Loss: 0.0412\n",
      "Epoch [46/75], Loss: 0.0511\n",
      "Epoch [47/75], Loss: 0.0480\n",
      "Epoch [48/75], Loss: 0.0370\n",
      "Epoch [49/75], Loss: 0.0333\n",
      "Epoch [50/75], Loss: 0.0265\n",
      "Epoch [51/75], Loss: 0.0275\n",
      "Epoch [52/75], Loss: 0.0344\n",
      "Epoch [53/75], Loss: 0.0316\n",
      "Epoch [54/75], Loss: 0.0268\n",
      "Epoch [55/75], Loss: 0.0238\n",
      "Epoch [56/75], Loss: 0.0328\n",
      "Epoch [57/75], Loss: 0.0329\n",
      "Epoch [58/75], Loss: 0.0285\n",
      "Epoch [59/75], Loss: 0.0249\n",
      "Epoch [60/75], Loss: 0.0216\n",
      "Epoch [61/75], Loss: 0.0182\n",
      "Epoch [62/75], Loss: 0.0204\n",
      "Epoch [63/75], Loss: 0.0307\n",
      "Epoch [64/75], Loss: 0.0339\n",
      "Epoch [65/75], Loss: 0.0292\n",
      "Epoch [66/75], Loss: 0.0292\n",
      "Epoch [67/75], Loss: 0.0276\n",
      "Epoch [68/75], Loss: 0.0242\n",
      "Epoch [69/75], Loss: 0.0242\n",
      "Epoch [70/75], Loss: 0.0170\n",
      "Epoch [71/75], Loss: 0.0167\n",
      "Epoch [72/75], Loss: 0.0201\n",
      "Epoch [73/75], Loss: 0.0188\n",
      "Epoch [74/75], Loss: 0.0241\n",
      "Epoch [75/75], Loss: 0.0225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# number of epochs\n",
    "num_epochs = 75\n",
    "# learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# train model\n",
    "model_tr, loss_all_epochs, accuracy = training_lstm(rnn, train_dataloader, valid_dataloader, num_epochs, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a73d3a",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91253967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "epochs = [i for i in range(num_epochs)]\n",
    "plt.plot(epochs, loss_all_epochs, 'r', label='Loss')\n",
    "plt.xlabel('epochs'), plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5c5f2",
   "metadata": {},
   "source": [
    "### Eval Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "011caa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, preds = eval_lstm(model_tr,test_dataloader,True)\n",
    "outputs=[]\n",
    "pred_labels=[]\n",
    "true_labels = []\n",
    "for o,p,t in preds:\n",
    "    outputs.extend(o)\n",
    "    pred_labels.extend(p)\n",
    "    true_labels.extend(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f021b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           B       0.54      0.50      0.52       328\n",
      "           I       0.67      0.55      0.61       339\n",
      "           O       0.87      0.91      0.89      1801\n",
      "\n",
      "    accuracy                           0.81      2468\n",
      "   macro avg       0.69      0.66      0.67      2468\n",
      "weighted avg       0.80      0.81      0.80      2468\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_labels,true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33614a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \u001b[42m[O][O]\u001b[0m goals \u001b[42m[O][O]\u001b[0m of \u001b[42m[O][O]\u001b[0m this \u001b[42m[O][O]\u001b[0m work \u001b[42m[O][O]\u001b[0m is \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[O][O]\u001b[0m to \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[O][O]\u001b[0m the \u001b[42m[O][O]\u001b[0m most \u001b[42m[O][O]\u001b[0m relevant \u001b[42m[O][O]\u001b[0m features \u001b[42m[O][O]\u001b[0m for \u001b[42m[O][O]\u001b[0m this \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[O]\u001b[0m\u001b[41m[B]\u001b[0m distinction \u001b[42m[O]\u001b[0m\u001b[41m[I]\u001b[0m and \u001b[42m[O][O]\u001b[0m second \u001b[42m[O][O]\u001b[0m to \u001b[42m[O][O]\u001b[0m build \u001b[42m[O][O]\u001b[0m an \u001b[42m[O][O]\u001b[0m automatic \u001b[42m[B][B]\u001b[0m UNK \u001b[42m[O]\u001b[0m\u001b[41m[I]\u001b[0m system \u001b[42m[O]\u001b[0m\u001b[41m[I]\u001b[0m of \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[O]\u001b[0m\u001b[41m[B]\u001b[0m UNK \u001b[42m[O]\u001b[0m\u001b[41m[I]\u001b[0m according \u001b[42m[O][O]\u001b[0m to \u001b[42m[O][O]\u001b[0m their \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[B][B]\u001b[0m\n",
      "\n",
      "\n",
      "The \u001b[42m[O][O]\u001b[0m approach \u001b[42m[O][O]\u001b[0m obtained \u001b[42m[O][O]\u001b[0m state \u001b[42m[B][B]\u001b[0m of \u001b[42m[I][I]\u001b[0m the \u001b[42m[I][I]\u001b[0m art \u001b[42m[O]\u001b[0m\u001b[41m[I]\u001b[0m results \u001b[42m[O][O]\u001b[0m by \u001b[42m[O][O]\u001b[0m combining \u001b[42m[O][O]\u001b[0m several \u001b[42m[O][O]\u001b[0m string \u001b[42m[B][B]\u001b[0m kernels \u001b[42m[I][I]\u001b[0m using \u001b[42m[O][O]\u001b[0m multiple \u001b[42m[B][B]\u001b[0m UNK \u001b[42m[I][I]\u001b[0m learning \u001b[42m[I][I]\u001b[0m\n",
      "\n",
      "\n",
      "UNK \u001b[42m[O][O]\u001b[0m the \u001b[42m[O][O]\u001b[0m presentation \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[O][O]\u001b[0m examples \u001b[42m[O][O]\u001b[0m are \u001b[42m[O][O]\u001b[0m used \u001b[42m[O][O]\u001b[0m for \u001b[42m[O][O]\u001b[0m UNK \u001b[42m[O][O]\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, out in enumerate(outputs[:3]):\n",
    "    print(out)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe70aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
